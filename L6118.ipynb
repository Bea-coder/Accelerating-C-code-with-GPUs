{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6118 - Simple Steps to Speed Up Your C Code Dramatically with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Abstract:\n",
    "Many scientists have developed their own old trusted C code, and they would like to make it run faster using the new GPU technology. However, they don't how to start this process. In this tutorial, we provide simple steps to speed up a piece of C code. To do so, we have selected the most common algorithms where the parallel architecture benefits are best and we describe the little details encountered in the transition from C to CUDA programming. \n",
    "\n",
    "This tutorial will cover how to do:\n",
    "    * Allocate Dynamic memory in the device \n",
    "    * Call functions \n",
    "    * Data transfer from device to host and host to device\n",
    "    * Execution of kernels\n",
    "    * Implementation of averages and histograms\n",
    "    \n",
    "We organize the tutorial in the following manner:\n",
    "    * Review of CUDA architecture and parallel programming.\n",
    "    * Reduce and Scan Algorithms\n",
    "    * Examples and Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key components in migrating from C to CUDA code (for a beginner) is to identify what implementation can be executed easily and will reduce the computation time significantly. To this aim,  it is necessary to learn the basics of CUDA architecture and a set of parallel algorithms which are frequently used and relatively easy to implement. This will help to decrease the slope of our learning curve and gradually deepen in the CUDA parallel world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the serial world of programming, we love loops because they allow us to solve problems by performing operations sequencially. In addition, they are easy to understand, implement, and debug. However, performing sequential  operations kills our performance when the size of the data to analyze and/or operations within the loops increases.\n",
    "In short, parallel coding represents \"breaking inefficient loops\". Here, we present two algorithms REDUCE and SCAN that allow us to break loops and exploit the parallel power of our GPUs, but first let's summarize how to call C functions, CUDA kernels, allocating device memory, and launching kernels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this tutorial, we provide C and CUDA templates and data files.   \n",
    "Our examples and exercises will have the following file organization. Under exercise or example directory, you will find:\n",
    "\n",
    "io.c       will contain functions that will read/write input/outfiles ( C functions)\n",
    "main.c     will contain the main C function\n",
    "main.cu    will contain the main CUDA function\n",
    "func.c     will contain extra-c functions\n",
    "kernels.cu will contain the kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASICS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, the main function has to be in a .cu file, and to be able to call CUDA functions, we need to add the include for the CUDA header file:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <cuda_runtime.h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaration of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to call a \"C\" function in the CUDA main function that does not use any CUDA class or function and it is located in other C file, it has to be declared as,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extern \"C\" type function(type parameters);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to call a user made CUDA function (a function that uses any CUDA class) and that it is not a kernel and it has to be located in other *.cu file, it has to be declared like, "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "type function (type parameters);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to call a Kernel that is outside of the main.cu file, it has to be declared as,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "__global__ void function(type parameters);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make your code suitable for different GPU, it is important to code base on the characteristics of any GPU. We can obtain this information by using the CUDA class __cudaDeviceProp__ and CUDA functions. Among them, we have \n",
    "- __cudaGetDeviceCount__ retuns the number of GPUs in your node or machine.\n",
    "- __cudaGetDevice(&dev)___ returns the device on which the active host thread executes the device code in dev.\n",
    "- __cudaGetDeviceProperties(&deviceName,dev)__ returns in *deviceName the properties of device dev.\n",
    "\n",
    "In addition, the __cudaDeviceProp__ class has many member elements such as:\n",
    "- __maxThreadsPerBlock__ gives the maximum number of threads per block that your GPU can handle.\n",
    "- __totalGlobalMem__ gives the total size of Global memory.\n",
    "- __sharedMemPerBlock__ gives the total size of Shared memory in each block.\n",
    "  \n",
    "among other features. We have extracted the main properties in a CUDA function __\"void printDevProp(cudaDeviceProp devProp)\"__, a more extended version of this function can be obtain in ~/NVIDIA_CUDA-7.5_Samples/1_Utilities/deviceQuery/deviceQuery.cpp. We also provided an small program cuda-features that runs this function.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, let's test what we have learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Given the function  \" void printDevProp(cudaDeviceProp devProp);\" in kernels.cu. Fill in the blanks in main.c file, so that typing: \"nvcc main.cu kernel.cu\" -o main gives no errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files are in  basics/exercise1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls basics/exercise1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=#include <stdio.h>\r\n",
      "#include <math.h>\r\n",
      "\r\n",
      "\r\n",
      "void printDevProp(cudaDeviceProp devProp)\r\n",
      "{\r\n",
      "    printf(\"\\n\\nGPU features:\");\r\n",
      "    printf(\"\\n\\tMajor revision number:         %d\\n\",  devProp.major);\r\n",
      "    printf(\"\\tMinor revision number:         %d\\n\",  devProp.minor);\r\n",
      "    printf(\"\\tName:                          %s\\n\",  devProp.name);\r\n",
      "    printf(\"\\tTotal global memory:           %lu kB\\tTotal %lu Kilo floats\\n\",  devProp.totalGlobalMem/1024,devProp.totalGlobalMem/(sizeof(float)*1000));\r\n",
      "    printf(\"\\tTotal shared memory per block: %lu kB\\t\\tTotal %lu Kilo floats\\n\",  devProp.sharedMemPerBlock/1024,devProp.sharedMemPerBlock/(sizeof(float)*1000));\r\n",
      "    printf(\"\\tTotal registers per block:     %d\\n\",  devProp.regsPerBlock);\r\n",
      "    printf(\"\\tWarp size:                     %d\\n\",  devProp.warpSize);\r\n",
      "    printf(\"\\tMaximum memory pitch:          %lu\\n\",  devProp.memPitch);\r\n",
      "    printf(\"\\tMaximum threads per block:     %d\\n\",  devProp.maxThreadsPerBlock);\r\n",
      "    for (int i = 0; i < 3; ++i)\r\n",
      "        printf(\"\\tMaximum dimension %d of block:  %d\\n\", i, devProp.maxThreadsDim[i]);\r\n",
      "\u001b[7mbasics/exercise1/kernels.cu \u001b[m\u001b[K"
     ]
    }
   ],
   "source": [
    "!more basics/exercise1/kernels.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=/* program that prints the GPU features*/ \r\n",
      "\r\n",
      "#include <stdio.h>\r\n",
      "\r\n",
      "\r\n",
      "int main(int argc, char *argv[])\r\n",
      "{\r\n",
      "\r\n",
      "  cudaDeviceProp prop;\r\n",
      "  int device;\r\n",
      "\r\n",
      "  cudaGetDevice(&device);\r\n",
      "  cudaGetDeviceProperties (&prop,device);\r\n",
      "  printDevProp(prop); \r\n",
      "\r\n",
      "\r\n",
      "  return(0); \r\n",
      "}\r\n",
      "\r\n",
      "\r",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "!more basics/exercise1/main.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, you corrected the code. Compile it by using nvcc main.cu kernels.cu -o main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunching Kernels and Device memory Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, that we know how to compile and call functions, let's learn how to luch kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kernels__ are sequences of instructions that are going to be processed by each thread. They are written similarly to C functions, but they usually have a prefix  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "__global__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and they are called or \"launched\" in the following manner,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kernel_name <<<number of blocks, number of threads, amount of share memory>>> (input parameters);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Blocks__ allow us to identify a set of threads that are going to be processed by a single SM, and thus they have access to the same share memory. The number of blocks and number of threads can be tailor to the design of your program, but their maximum value is specific of each GPU. In the previous exercise, we show how to obtain parameters like the maximum number of threads per blocks, the maximum number of threads in a particular dimension, among others. The blocks can be arranged in arrays of 1, 2 ,3 dimensions, the specifications of the arrangement of blocks is called __Grid dimensionality__.\n",
    "\n",
    "__Threads__ are our engines to perform operations through kernels. They have their unique Id, base on the number of threads per block and the number of blocks. They also can be 1D, 2D, or 3D, and for these examples, we are going to use only 1D arrangement of threads per block.\n",
    "\n",
    "__Input parameters__  If they are arrays, they have to be allocated specifically for the GPU/device. Since most of the programs allocate memory dynamically, we are going to focus on such. One main difference in CUDA programming is device's arrays can not be accessed by the host or CPU. In other to have access, we have to transfer memory from device to host and vice-versa. CUDA has specific functions to do that such as,   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cudaMemcpy(destination,source,size_array, cudaMemcpyDeviceToHost);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cudaMemcpy(destination,source,size_array, cudaMemcpyHostToDevice);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both arrays have to have the same size. \n",
    "To allocate memory for the device, we use __cudaMalloc__, which requires to specify the size of the array and the type like,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type *d_array =NULL;\n",
    "unsigned int size_array=sizeof(type)*nblocks;\n",
    "cudaMalloc((void **)&d_array,size_array);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where, __d_array__ is the name of the array and  __size_array__ is the size of array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this summary of CUDA programming, let's launch some kernels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A.__ Generate an array of 10000 numbers, with values increasing from 1 to 10000. For this part of the exercise, we provide the kernel, but you have to add the missing parts in the main."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The code is under basics/exercise2/partA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__global__ void int_generator (int *d_out,unsigned long int size){\r\n",
      "\r\n",
      "    int index_x = blockIdx.x * blockDim.x + threadIdx.x;\r\n",
      "    int index_y = blockIdx.y * blockDim.y + threadIdx.y;\r\n",
      "    int grid_width = gridDim.x * blockDim.x;\r\n",
      "\r\n",
      "    /* map the two 2D indices to a single linear, 1D index */\r\n",
      "\r\n",
      "    int gId  = index_y * grid_width + index_x;\r\n",
      "    if(gId<size){\r\n",
      "      d_out[gId]=gId;\r\n",
      "    }     \r\n",
      "\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    " tail -14 basics/exercise2/partA/kernerls.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B__. Using the example in part A, generate 100 arrays, where each array will have increasing values from 1 to 1000.\n",
    "The C and CUDA templates for this exercise are: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "io.c       has a c function that will write an array into a file\n",
    "kernels.cu and main.cu are templates where you can fill in the blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to lunch kernel, we are ready to learn basic parallel algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDUCE: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce as its name suggests is a primitive or function that reduces many inputs to one or more outputs by performing an associative operation on the inputs. __Average__ is an ideal case where it can be used. Let's go further on the details of its implementation. \n",
    "\n",
    "The serial version of computing the average is,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "average=0;\n",
    "for(int i=0;i<array_size;i++)\n",
    " average+=array[i]/(float)array_size;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of implementing this algorithm is that each operation depends on a previous one. So, if we want to implement this approach in a GPU, each thread would have to access the result of the previous one, thus they can be computed at the same time, which is not good.\n",
    "\n",
    "We need to find the way of obtaining the same output, by distributing operations that do not depend on the results of others and do not need to have access to the same memory location, so that they can be computed in parallel. \n",
    "\n",
    "To break this for loop into independent operations, the previous scheme won't help much; however, if we expand the array in a different manner, we can express the same sum by accessing different locations at once. For example, if __a__ is an array of size of 8, its average is\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "average=(a[0]+a[1]+a[2]+a[3]+a[4]+a[5]+a[6]+a[7])/8;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we associate them and accumulate its values at different steps, such as"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Step 0\n",
    "a[0]=a[0]+a[4];\n",
    "a[1]=a[1]+a[5];\n",
    "a[2]=a[2]+a[6];\n",
    "a[3]=a[3]+a[7];\n",
    "\n",
    "#Step 1\n",
    "a[0]=a[0]+a[2];\n",
    "a[1]=a[1]+a[3];\n",
    "\n",
    "#Step 2\n",
    "a[0]=a[0]+a[1];\n",
    "\n",
    "average=a[0]/8;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a[0]__ will have all the terms of the array. The previous arrangement can be represented in C code as,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for (jump=size_array/2;jump>0;jump=jump/2)\n",
    "     array[i]+=array[i+jump];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look in CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example1: Computing the average of a set of numbers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we are going to use the program that we implemented in the last exercise, but in addition, we are going to compute the average of the 10000 numbers generated. The kernel that is going to performed the average is called __reduce__,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__global__ void reduce (float *d_in,float *d_out,unsigned long int size){\r\n",
      "\r\n",
      "    extern __shared__ float sdata[];\r\n",
      "\r\n",
      "   \r\n",
      "    // map the two 2D indices to a single linear, 1D index\r\n",
      "    int gId  = blockIdx.x*blockDim.x +threadIdx.x;\r\n",
      "    int lId  = threadIdx.x;\r\n",
      "    int bId  = blockIdx.y * gridDim.x + blockIdx.x;\r\n",
      "\r\n",
      "   if(gId<size)\r\n",
      "    sdata[lId]=d_in[gId];\r\n",
      "   else\r\n",
      "    sdata[lId]=0.0;\r\n",
      "\r\n",
      "    __syncthreads();    \r\n",
      "\r\n",
      "   for(unsigned int s =blockDim.x/2;s>0;s >>=1){\r\n",
      "\r\n",
      "        if(lId<s){\r\n",
      "\t sdata[lId]+=sdata[lId+s];\r\n",
      "        }\r\n",
      "    __syncthreads();\r\n",
      "\r\n",
      "   }\r\n",
      "    if(lId==0){\r\n",
      "      d_out[bId]=sdata[0];\r\n",
      "    }     \r\n",
      "\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "head -44 reduce/example1/kernerls.cu |tail -30 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, we have two new expressions, __extern__  ___shared___ __ __ and ____syncthreads()__.  \n",
    "\n",
    "__extern__ ___shared___ is used to declare the share memory that is going to be used in the Kernel. We could use Global memory instead, but the access to the share memory is faster than to the global memory. So, it will speed up our code. \n",
    "\n",
    "____syncthreads()__ is a command that holds the execution of the kernel until all threads of the block reach that point. We use it here to __wait until__ all the threads copy the values of the array from global memory to share memory. To then perform the for-loop."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now let's implement it into a bit more complicated exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise1. Computing the average visited sites of a Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute numerically the time dependence of the average visitted sites of a random walk in a 3D lattice $\\langle W\\rangle$. \n",
    "\n",
    "There are many ways of solving this problem, however we are going to solve by simply marking and counting the places where the random walk has jumped in time and average them. Although the concept is very simple, it will require to add a 3D matrix at every time step $t$. Thus, the time increases, the computational time is increases approximately $t*N_{x}N_{y}N_{z}$, where $N_i$ is the maximum distance in the x,y, or z direction. \n",
    "\n",
    "The files for this exercise is located in reduce/exercise1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To aid its implementation, we provide the C code in main.c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  printf(\"\\nComputing Visited sites....     \");\r\n",
      "  for(i=0;i<nBlocks;i++){\r\n",
      "    printf(\"\\b\\b\\b\\b%3.0f%%\",100.0*(double)i/(double)nBlocks);\r\n",
      "    fflush(NULL);\r\n",
      "    zero(space,size);\r\n",
      "\r\n",
      "    for(n=0;n<sizeBlock;n++){\r\n",
      "       for(k=0;k<NDIM;k++){\r\n",
      "          mark[k]=(int)ceil(abs(x[(i*sizeBlock+n)*NDIM+k]-min[k])); \r\n",
      "        }              \r\n",
      "        space[mark[0]+mark[1]*size[0]+mark[2]*size[0]*size[1]]=1;\r\n",
      "        ave_vol[n]+=sumSpace(space,size);\r\n",
      "   }\r\n",
      "  }\r\n",
      "  \r\n"
     ]
    }
   ],
   "source": [
    "head -75 reduce/exercise1/Ccode/main.c|tail -15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trajectory of the particle is in the filename: random3Dt.xyz. The structure of the file is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000   \r\n",
      "144.97984\t-55.087448\r\n",
      "2.232421\t-137.551697\r\n",
      "168.748016\t-0.000083\r\n",
      "#time x y z \r\n",
      "1 0.0 0.0 0.0\r\n",
      "2 0.036407981837455534 0.038226280215159406 0.03640427649996404\r\n",
      "3 -0.06905383848196306 0.058088535372306946 0.05241198500306482\r\n",
      "4 -0.11965267621336795 0.0550593544445928 -0.05898333989857282\r\n",
      "5 -0.0942907350638626 0.2754845522401567 -0.06168650892253492\r\n"
     ]
    }
   ],
   "source": [
    "head -10 reduce/random3Dt.xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the io.c file, you will find the functions to read and extract the positions of the file and they are already written at the beginning of the main.cu file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use the template main.cu, and the files: kernels.cu, io.c to implement the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!cd reduce/exercise1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scan algorithm is one of the building blocks for many parallel algorithms. The __inclusive scan__ is defined as for  a given input array, it generates a new array where each element $j$ of the resulting array is the sum of all elements up to and including the $j$ of the input array. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For instance, the serial implementation is:\n",
    "\n",
    "b[0]=a[0];\n",
    "for(int i=1;i<size_array;i++)\n",
    "   b[i]+=b[i-1]+a[i];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways of implementing this algorithm, we are going to start with the most simple to understand and easy to implement for practical matter.  \n",
    "\n",
    "At first, it seems that we cannot separate those operations. To picture better algorithm, let's expand the result for an array of 8 elements, __b__ would be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b[0]=a[0]\n",
    "b[1]=a[0]+a[1];\n",
    "b[2]=a[0]+a[1]+a[2];\n",
    "b[3]=a[0]+a[1]+a[2]+a[3];\n",
    "b[4]=a[0]+a[1]+a[2]+a[3]+a[4];\n",
    "b[5]=a[0]+a[1]+a[2]+a[3]+a[4]+a[5];\n",
    "b[6]=a[0]+a[1]+a[2]+a[3]+a[4]+a[5]+a[6];\n",
    "b[7]=a[0]+a[1]+a[2]+a[3]+a[4]+a[5]+a[6]+a[7];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can separate this sum by adding each element $i$ the previous $i-2^{i}$ elements, for elements bigger and equal than $2^{i}$ at different steps.  The C code of such implementation would be"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for(int i=0;i<size_array;i++)\n",
    "  b[i]=a[i];\n",
    "\n",
    "for(int i=0;i<log(size_array);i++)\n",
    "   if(i>=pow(2,1))\n",
    "     b[i]+=b[i-pow(2,i)];\n",
    "\n",
    "For instance, the element 5;\n",
    "\n",
    "b[5]=a[5]; //initialization\n",
    "\n",
    "b[5]=a[5]+b[4]=a[5]+a[4]; //at i=0\n",
    "\n",
    "b[5]=a[5]+a[4]+b[3]=a[5]+a[4]+a[3]+a[2]; //at i=1\n",
    "\n",
    "b[5]=a[5]+a[4]+a[3]+a[2]+b[1]=a[5]+a[4]+a[3]+a[2]+a[1]+a[0] //at i=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note. The number of steps went down to log(size)=log(8)=3, instead of 8.\n",
    "Now, let's implement it within an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to implement the scan operation in a program that generates an array of N elements with a given value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scan/example1/main 10 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we generate an array of 20 elements with a value of 1. The output will be,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=1.000000\r\n",
      "2.000000\r\n",
      "3.000000\r\n",
      "4.000000\r\n",
      "5.000000\r\n",
      "6.000000\r\n",
      "7.000000\r\n",
      "8.000000\r\n",
      "9.000000\r\n",
      "10.000000\r\n",
      "11.000000\r\n",
      "12.000000\r\n",
      "13.000000\r\n",
      "14.000000\r\n",
      "15.000000\r\n",
      "16.000000\r\n",
      "17.000000\r\n",
      "18.000000\r\n",
      "19.000000\r\n",
      "20.000000\r\n",
      "\r",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "more scan/example1/output.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the __scan__ algorithm in two kernels called __scanA__ and __scanB__. The __scanA__ will copy the values that are not going to be change at any step to the resulting array, and the __scanB__ will add the values of the element $i-2^{i}$ to the $i$ element each $i$ step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__global__ void scan_B(float *x,float *dx,unsigned long int size,int jump){\r\n",
      "\r\n",
      "    int gId = (blockIdx.y*gridDim.x+blockIdx.x)*blockDim.x+threadIdx.x;\r\n",
      " \r\n",
      "    float x0,x1;\r\n",
      "\r\n",
      "       x0=x[gId];\r\n",
      "       __syncthreads();\r\n",
      "\r\n",
      "       x1=x[gId+jump];\r\n",
      "       __syncthreads();\r\n",
      "\r\n",
      "\r\n",
      "       if((gId+jump)<size)\r\n",
      "        dx[gId+jump]=x1+x0;\r\n",
      "}\r\n",
      "\r\n",
      "__global__ void scan_A(float *x,float *dx,unsigned long int size){\r\n",
      "\r\n",
      "    int gId = (blockIdx.y*gridDim.x+blockIdx.x)*blockDim.x+threadIdx.x;\r\n",
      "\r\n",
      "    float x0;\r\n",
      "\r\n",
      "       x0=x[gId];\r\n",
      "\r\n",
      "      if(gId<size)\r\n",
      "        dx[gId]=x0;\r\n",
      "\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "head -39 scan/example1/kernerls.cu|tail -29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement it a more practical exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to implement a simple code that computes the time average of the second moment of a random variable $\\langle x^{2}\\rangle$. We will provide the entire C routine, and you will implement it in a template CUDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the second moment of a random variable \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the implementation more significant, we are going to analyze the positions of a particle that moves in 3D randomly. Thus, the time average of the second moment is the variation in time of the mean square displacement of the particle __msd(t)__ which for a Brownian motion, __msd(t)__ is linearly proportional to __t__ .\n",
    "\n",
    "For this example, we provide the trajectory of the particle, which is in file: random3Dt.xyz\n",
    "\n",
    "First, we are going to explain how it would be in C code, then we will convert it in CUDA.\n",
    "\n",
    "You will find all files in the scan/exercise1 directory "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The structure of the trajectory file random3Dt.xyz is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000   \r\n",
      "#time x y z   \r\n",
      "1 0.0 0.0 0.0\r\n",
      "2 0.036407981837455534 0.038226280215159406 0.03640427649996404\r\n",
      "3 -0.06905383848196306 0.058088535372306946 0.05241198500306482\r\n",
      "4 -0.11965267621336795 0.0550593544445928 -0.05898333989857282\r\n",
      "5 -0.0942907350638626 0.2754845522401567 -0.06168650892253492\r\n",
      "6 -0.20169277539001323 0.44299126333386823 0.033296495974359915\r\n",
      "7 -0.2257369555712272 0.5597075775431412 0.023790254634477363\r\n",
      "8 -0.23419574273545854 0.6336599820864995 0.048494147655551274\r\n"
     ]
    }
   ],
   "source": [
    "head random3Dt.xyz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "where the first line gives the number of time-steps, the second line gives the information of the columns of the file, i.e. the first column is time, the second is position in x, the third is position in y, and the last one is the position in z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid its implementation, we provide the C code that computes msd(t). In addition, we divide in blocks our trajectory. For each block, we are going to compute the distance of the particle between the starting time of the block to each time-step. Then, we average the blocks at each time.\n",
    "\n",
    "The number of blocks (nbl) is going to be constant and is an input parameter, to simplify our example.\n",
    "\n",
    "The C code has the following structure\n",
    " \n",
    "    * Declaration of variables and functions\n",
    "    * Memory Allocation\n",
    "    * Initialization of variables\n",
    "    * Core of the program (performing the actual computation)\n",
    "    * Output file\n",
    "\n",
    "We are going to keep the same structure for our CUDA code example to emphasize the differences between C and CUDA. \n",
    "\n",
    "__Declaration of variables and functions__ are at the beginning of the program. To see them, we can type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int read_initial_parameters ( char filename[20],long int *nconf);\r\n",
      "int read_file( char filename[20],long int nconf, float *x,int *time);\r\n",
      "void write_output(char fname[MAX], long int nconf, float *msd, int *time,int size);\r\n",
      "\r\n",
      "int main(int argc, char *argv[])\r\n",
      "{\r\n",
      "  float *x, *msd, rtot[NDIM], r2;\r\n",
      "  int nbl,sizebl,*time;\r\n",
      "  long int nconf,t;\r\n",
      "  FILE *fp;\r\n",
      "  int i,k;\r\n",
      "  clock_t start, diff;\r\n",
      "\r\n",
      " if (argc < 3){\r\n",
      "    printf(\"Usage: msd_c <pos file> <nbl> \\n\");\r\n",
      "    return(0);\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "head -24 scan/exercise1/Ccode/main.c|tail -16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where,\n",
    "__read_initial_parameters__ obtains the number steps of the particle. \n",
    "__read_file__  opens the position file.\n",
    "__write_output__ save the msd as a function of time.\n",
    "\n",
    "Now, we focus in the core part of the program, computing the msd, which uses the following variables,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Vector variables:\n",
    "x, has the positions of the particle.\n",
    "time, has the sequential time-steps \n",
    "msd, has the computed msd.\n",
    "\n",
    "Scalar variables:\n",
    "nconf, number of total time-steps of the trajectory.\n",
    "nbl, the number of blocks that the trajectory is going to be split.\n",
    "sizebl, the size of each block, i.e. $sizebl=nconf/nbl$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see __the core__ of the program, we can type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  printf(\"\\nCaculating MSD...     \");\r\n",
      "  for (i=0; i<nbl; i++) {\r\n",
      "      printf(\"\\b\\b\\b\\b%3.0f%%\", 100.0*(float)i/(float)nbl);\r\n",
      "      fflush(NULL);      \r\n",
      "      r2=0;\r\n",
      "      for (t=1; t<sizebl; t++) {\r\n",
      "          rtot[0] = rtot[1] = rtot[2] = 0.0;\r\n",
      "\t  for (k=0; k<NDIM; k++){\r\n",
      "\t       rtot[k] += (x[(i*sizebl+t)*NDIM+k]-x[(i*sizebl+t-1)*NDIM+k]); \r\n",
      "               r2 += rtot[k]*rtot[k];\r\n",
      "\t  }  \r\n",
      "          msd[t] += r2;\r\n",
      "         \r\n",
      "      }\r\n",
      "  }\r\n"
     ]
    }
   ],
   "source": [
    "head -70 scan/exercise1/Ccode/main.c|tail -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This double loop is what we need to parallelize. In order to do so, we need to break the double for-loop. Note that msd[t] is the addition of all previous msd values, a direct application of the __Scan__ algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cd scan/exercise1/Ccode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile the C code, we write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "gcc main.c io.c -o main_c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To run the code, we type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: msd_c <pos file> <nbl> \r\n"
     ]
    }
   ],
   "source": [
    "./main_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not specify any parameters, the code will tell you what information you need to provide, where nbl is the size of the block, thus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In addition, we provide the templates for main.cu kernel.cu and io.c files as previous exercises in scan/exercise1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final exercise is to compute a histogram. A histogram counts the occurence of a value within a set of numbers. Only using scan and reduce, we can easily compute a histogram. \n",
    "\n",
    "Let's compute the histogram of the values generated in __exercise 2B__ from the __Basics__ section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways of computing histograms; a easy implementation is to reduce only the values that belong to each bin identifier. This method is called __reduce by key__. Our key is going to be the number of the bin that each value belongs. We first need to generate the keys for all values, then we reduce the ones that have the same value.\n",
    "\n",
    "Let's take a look of the generation of the key kernel,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__global__ void keyGenerator(float *d_in, int *bin, float binSize,unsigned long int size){\r\n",
      "    int index_x = blockIdx.x * blockDim.x + threadIdx.x;\r\n",
      "    int index_y = blockIdx.y * blockDim.y + threadIdx.y;\r\n",
      "    int grid_width = gridDim.x * blockDim.x;\r\n",
      "    int gId  = index_y * grid_width + index_x;\r\n",
      "    int key;\r\n",
      "    float local;\r\n",
      "\r\n",
      "    if(gId<size){\r\n",
      "     local=d_in[gId];\r\n",
      "     key = (int) (local/binSize);\r\n",
      "     bin[gId]=key;\r\n",
      "    }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "head -23 histo/example/kernerls.cu |tail -14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the __reducebykey__ kernel which has a small variation of the reduce kernel. We do not care about the value of the element to reduce, but the key value. If it is the same key that we are reducing, then we add 1 otherwise nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__global__ void reducebykey (int *bin, float *d_out,unsigned long int size,int key){\r\n",
      "\r\n",
      "    extern __shared__ float sdata[];\r\n",
      "\r\n",
      "    int index_x = blockIdx.x * blockDim.x + threadIdx.x;\r\n",
      "    int index_y = blockIdx.y * blockDim.y + threadIdx.y;\r\n",
      "    int grid_width = gridDim.x * blockDim.x;\r\n",
      "\r\n",
      "    // map the two 2D indices to a single linear, 1D index\r\n",
      "    int gId  = index_y * grid_width + index_x;\r\n",
      "    int lId = threadIdx.x;\r\n",
      "    int bId  = blockIdx.y * gridDim.x + blockIdx.x;\r\n",
      "    float filter;\r\n",
      "\r\n",
      "    int lkey = bin[gId];\r\n",
      "\r\n",
      "    if (lkey == key)\r\n",
      "       filter=1.0;\r\n",
      "    else\r\n",
      "      filter=0.0;\r\n",
      "   \r\n",
      "   if(gId<size){\r\n",
      "     sdata[lId]=filter;\r\n",
      "   }\r\n",
      "   else\r\n",
      "    sdata[lId]=0.0;\r\n",
      "\r\n",
      "    __syncthreads();    \r\n",
      "\r\n",
      "   for(unsigned int s =blockDim.x/2;s>0;s >>=1){\r\n",
      "\r\n",
      "        if(lId<s){\r\n",
      "\t sdata[lId]+=sdata[lId+s];\r\n",
      "        }\r\n",
      "    __syncthreads();\r\n",
      "\r\n",
      "   }\r\n",
      "    if(lId==0){\r\n",
      "      d_out[bId]=sdata[0];\r\n",
      "    }     \r\n",
      "\r\n",
      "}\r\n",
      "__global__ void reduce (float *d_in,float *d_out,unsigned long int size){\r\n"
     ]
    }
   ],
   "source": [
    "head -66 histo/example/kernerls.cu |tail -43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's practice in an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the histogram of the trajectory file random3Dt.xyz. \n",
    "Fill in the blanks of the main.cu and use the kernels.cu file form the example above. \n",
    "All files are located in __histo/exercise/__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
